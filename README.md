##Desarrollo tarea 1

<h1 align="center">ğŸ¤– Movimiento Aleatorio de un robot </h1>

  Este proyecto implementa una simulaciÃ³n sencilla de un robot en una grid **3x3**, donde el objetivo es que el robot llegue a la posiciÃ³n final **(3, 3)** optimizando su baterÃ­a y sus movimientos.

---

## ğŸ¯ Objetivo
El robot debe alcanzar la meta en la menor cantidad de pasos posibles, tomando decisiones que maximicen la recompensa y evitando quedar bloqueado por falta de baterÃ­a.

---

## âš¡ Estrategia del Robot

El robot sigue una **estrategia con aleatoriedad controlada**, es decir:
- **Movimiento:** puede avanzar **arriba, abajo, izquierda o derecha**.  
- **Recarga:** si la acciÃ³n elegida es `recargar`, la baterÃ­a se reinicia al **100%**.  
- **RestricciÃ³n de baterÃ­a:**  
  - Si la baterÃ­a es **> 20**, puede moverse (cada movimiento consume -10).  
  - Si la baterÃ­a es **â‰¤ 20**, no puede moverse y se le fuerza a recargar.  
- **Aleatoriedad:** en cada paso, la acciÃ³n se elige de manera aleatoria entre las posibles (`adelante`, `atrÃ¡s`, `izquierda`, `derecha`, `recargar`).  
- **CondiciÃ³n de Ã©xito:** cuando alcanza la posiciÃ³n `(3, 3)`, el robot detiene la ejecuciÃ³n.  

---

## ğŸ”‹ BaterÃ­a
- El valor inicial de baterÃ­a se genera de manera **aleatoria** entre **10 y 100**, en intervalos de 15:  
  `10, 25, 40, 55, 70, 85, 100`.  
- Cada movimiento cuesta **10 unidades de baterÃ­a**.  
- Recargar siempre devuelve la baterÃ­a a **100%**.

---

## ğŸ† Sistema de Recompensas
La recompensa estÃ¡ diseÃ±ada para guiar al robot hacia decisiones correctas:
- `+20` â†’ Si alcanza el objetivo.  
- `+5` â†’ Si decide recargar.  
- `-1` â†’ Si se mueve normalmente.  
- `-5` â†’ Si intenta moverse con poca baterÃ­a.  
- `-10` â†’ Si intenta moverse sin baterÃ­a (castigo mayor).  

Esto obliga al robot a **aprender una estrategia eficiente**, aunque las acciones se eligen aleatoriamente.

## ğŸš¦ Ejemplo de EjecuciÃ³n
```phyton
Total de estados posibles: 63
Estado inicial del robot: {'posicion': (0, 0), 'bateria': 70, 'objetivo_alcanzado': False}
ğŸ”‹ BaterÃ­a recargada preventivamente al 100%
Paso 1: AcciÃ³n = recargar, Estado = {'posicion': (0, 0), 'bateria': 85, 'objetivo_alcanzado': False}, Recompensa = 5
Paso 2: AcciÃ³n = adelante, Estado = {'posicion': (1, 0), 'bateria': 70, 'objetivo_alcanzado': False}, Recompensa = -1
Paso 3: AcciÃ³n = adelante, Estado = {'posicion': (2, 0), 'bateria': 55, 'objetivo_alcanzado': False}, Recompensa = -1
Paso 4: AcciÃ³n = adelante, Estado = {'posicion': (3, 0), 'bateria': 40, 'objetivo_alcanzado': False}, Recompensa = -1
Paso 5: AcciÃ³n = derecha, Estado = {'posicion': (3, 1), 'bateria': 25, 'objetivo_alcanzado': False}, Recompensa = -1
Paso 6: AcciÃ³n = derecha, Estado = {'posicion': (3, 2), 'bateria': 10, 'objetivo_alcanzado': False}, Recompensa = -5
ğŸ”‹ BaterÃ­a recargada al 100%
Paso 7: AcciÃ³n = recargar, Estado = {'posicion': (3, 2), 'bateria': 100, 'objetivo_alcanzado': False}, Recompensa = 5
Paso 8: AcciÃ³n = derecha, Estado = {'posicion': (3, 3), 'bateria': 85, 'objetivo_alcanzado': True}, Recompensa = 20

âœ… SimulaciÃ³n terminada: objetivo alcanzado en 8 pasos.

Recompensa total obtenida: 21

```

## ğŸ”‘ Puntos Clave
- **determinista**: la baterÃ­a controla cuÃ¡ndo puede moverse.  
- **Aleatoriedad** en las decisiones evita que el robot siga siempre el mismo camino. 

---

## ğŸš€ Posibles Mejoras
- Implementar aprendizaje por refuerzo para optimizar la recompensa a largo plazo.  
- Ampliar el tamaÃ±o de la grilla o introducir obstÃ¡culos.
